{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [
    "Hhr_-zGIntNL"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **TL;DR**\n",
    "\n",
    "Use a pretraind a Vision Transformer provided by [`lightly`](https://github.com/lightly-ai/lightly) to create a vector index on an arbitrary dataset for Image Retrieval using [`faiss`](https://github.com/facebookresearch/faiss)."
   ],
   "metadata": {
    "id": "4S5P65xy3i3x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“¦ Packages and Basic Setup\n",
    "---"
   ],
   "metadata": {
    "id": "Sssfm1GmkgmT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WK-uaF3eJ9ay"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "%%capture\n",
    "!pip install faiss-gpu lightning lightly datasets supervision wandb ml-collections -q\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import warnings\n",
    "from typing import Any, Callable, Dict, List, Tuple, TypeAlias, Union\n",
    "\n",
    "import cv2\n",
    "import faiss\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from datasets import load_dataset\n",
    "from faiss import read_index, write_index\n",
    "from PIL import Image, ImageOps\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor\n",
    "from torch.nn import Identity\n",
    "from torch.optim import SGD\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchvision.models import resnet50\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import (\n",
    "    activate_requires_grad,\n",
    "    deactivate_requires_grad,\n",
    "    get_weight_decay_parameters,\n",
    "    update_momentum,\n",
    ")\n",
    "from lightly.transforms import DINOTransform\n",
    "from lightly.utils.benchmarking import OnlineLinearClassifier\n",
    "from lightly.utils.scheduler import CosineWarmupScheduler, cosine_schedule\n",
    "\n",
    "BATCH_TYPE: TypeAlias = Dict[str, Any]\n",
    "\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "key = userdata.get(\"W&B\")\n",
    "os.environ[\"WANDB_API_KEY\"] = key\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "!wget https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/checkpoints/epoch%3D99-step%3D1000900.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import ml_collections\n",
    "\n",
    "# @title âš™ï¸ Configuration\n",
    "import wandb\n",
    "\n",
    "\n",
    "def get_config() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.dataset: str = \"ethz/food101\"  # @param {type: \"string\"}\n",
    "    config.method: str = \"dino\"  # @param {type: \"string\"}\n",
    "    config.lr: float = 0.001  # @param {type: \"number\"}\n",
    "    config.epochs: int = 10  # @param {type: \"number\"}\n",
    "    config.batch_size: int = 32  # @param {type: \"number\"}\n",
    "    config.top_k: int = 3  # @param {type: \"number\"}\n",
    "    config.random_seed: int = 42  # @param {type: \"number\"}\n",
    "    config.upload_to_wandb = True  # @param {type:\"boolean\"}\n",
    "    config.fetch_from_wandb = True  # @param {type:\"boolean\"}\n",
    "    config.upload_to_hf = True  # @param {type:\"boolean\"}\n",
    "    config.hf_entity: str = \"sauravmaheshkar\"  # @param {type: \"string\"}\n",
    "    config.wandb_entity: str = \"sauravmaheshkar\"  # @param {type: \"string\"}\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "WJieMscJmgtX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ’¿ The Dataset\n",
    "---\n",
    "\n",
    "In this particular example we use the `ethz/food101` dataset from the huggingface hub. This dataset contains 75750 images from 101 classes.\n",
    "\n",
    "* This snippet highlights the ease of use of off-the-shelf `lightly` transforms ([`DINOTransform`](https://docs.lightly.ai/self-supervised-learning/lightly.transforms.html#lightly.transforms.dino_transform.DINOTransform)) when used with the huggingface ecosytem.\n",
    "* No need to refactor images into a single folder, can simply iterate over all images in the dataset object."
   ],
   "metadata": {
    "id": "zIc9K3KJmmSV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "dataset = load_dataset(config.dataset, trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": true,
    "id": "Ki-pwvRSLAll"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## âœï¸ Model Architecture & Training\n",
    "---\n",
    "\n",
    "![](https://github.com/SauravMaheshkar/SauravMaheshkar/blob/main/assets/DINO/DINO.png?raw=true)"
   ],
   "metadata": {
    "id": "kk1S5GNowcmw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "\n",
    "\n",
    "class DINO(LightningModule):\n",
    "    def __init__(self, batch_size_per_device: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size_per_device = batch_size_per_device\n",
    "\n",
    "        resnet = resnet50()\n",
    "        resnet.fc = Identity()  # Ignore classification head\n",
    "        self.backbone = resnet\n",
    "        self.projection_head = DINOProjectionHead(freeze_last_layer=1)\n",
    "        self.student_backbone = copy.deepcopy(self.backbone)\n",
    "        self.student_projection_head = DINOProjectionHead()\n",
    "        self.criterion = DINOLoss()\n",
    "\n",
    "        self.online_classifier = OnlineLinearClassifier(num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def forward_student(self, x: Tensor) -> Tensor:\n",
    "        features = self.student_backbone(x).flatten(start_dim=1)\n",
    "        projections = self.student_projection_head(features)\n",
    "        return projections\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        deactivate_requires_grad(self.backbone)\n",
    "        deactivate_requires_grad(self.projection_head)\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        activate_requires_grad(self.backbone)\n",
    "        activate_requires_grad(self.projection_head)\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[List[Tensor], Tensor, List[str]], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # Momentum update teacher.\n",
    "        momentum = cosine_schedule(\n",
    "            step=self.trainer.global_step,\n",
    "            max_steps=self.trainer.estimated_stepping_batches,\n",
    "            start_value=0.996,\n",
    "            end_value=1.0,\n",
    "        )\n",
    "        update_momentum(self.student_backbone, self.backbone, m=momentum)\n",
    "        update_momentum(self.student_projection_head, self.projection_head, m=momentum)\n",
    "\n",
    "        views, targets = batch[0], batch[1]\n",
    "        global_views = torch.cat(views[:2])\n",
    "        local_views = torch.cat(views[2:])\n",
    "\n",
    "        teacher_features = self.forward(global_views).flatten(start_dim=1)\n",
    "        teacher_projections = self.projection_head(teacher_features)\n",
    "        student_projections = torch.cat(\n",
    "            [self.forward_student(global_views), self.forward_student(local_views)]\n",
    "        )\n",
    "\n",
    "        loss = self.criterion(\n",
    "            teacher_out=teacher_projections.chunk(2),\n",
    "            student_out=student_projections.chunk(len(views)),\n",
    "            epoch=self.current_epoch,\n",
    "        )\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"ema_momentum\": momentum},\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            batch_size=len(targets),\n",
    "        )\n",
    "\n",
    "        # Online classification.\n",
    "        cls_loss, cls_log = self.online_classifier.training_step(\n",
    "            (teacher_features.chunk(2)[0].detach(), targets), batch_idx\n",
    "        )\n",
    "        self.log_dict(cls_log, sync_dist=True, batch_size=len(targets))\n",
    "        return loss + cls_loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[Tensor, Tensor, List[str]], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        images, targets = batch[0], batch[1]\n",
    "        features = self.forward(images).flatten(start_dim=1)\n",
    "        cls_loss, cls_log = self.online_classifier.validation_step(\n",
    "            (features.detach(), targets), batch_idx\n",
    "        )\n",
    "        self.log_dict(cls_log, prog_bar=True, sync_dist=True, batch_size=len(targets))\n",
    "        return cls_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Don't use weight decay for batch norm, bias parameters, and classification\n",
    "        # head to improve performance.\n",
    "        params, params_no_weight_decay = get_weight_decay_parameters(\n",
    "            [self.student_backbone, self.student_projection_head]\n",
    "        )\n",
    "        # For ResNet50 we use SGD instead of AdamW/LARS as recommended by the authors:\n",
    "        # https://github.com/facebookresearch/dino#resnet-50-and-other-convnets-trainings\n",
    "        optimizer = SGD(\n",
    "            [\n",
    "                {\"name\": \"dino\", \"params\": params},\n",
    "                {\n",
    "                    \"name\": \"dino_no_weight_decay\",\n",
    "                    \"params\": params_no_weight_decay,\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"online_classifier\",\n",
    "                    \"params\": self.online_classifier.parameters(),\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ],\n",
    "            lr=0.03 * self.batch_size_per_device * self.trainer.world_size / 256,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": CosineWarmupScheduler(\n",
    "                optimizer=optimizer,\n",
    "                warmup_epochs=int(\n",
    "                    self.trainer.estimated_stepping_batches\n",
    "                    / self.trainer.max_epochs\n",
    "                    * 10\n",
    "                ),\n",
    "                max_epochs=int(self.trainer.estimated_stepping_batches),\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def configure_gradient_clipping(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        gradient_clip_val: Union[int, float, None] = None,\n",
    "        gradient_clip_algorithm: Union[str, None] = None,\n",
    "    ) -> None:\n",
    "        self.clip_gradients(\n",
    "            optimizer=optimizer,\n",
    "            gradient_clip_val=3.0,\n",
    "            gradient_clip_algorithm=\"norm\",\n",
    "        )\n",
    "        self.student_projection_head.cancel_last_layer_gradients(self.current_epoch)\n",
    "\n",
    "\n",
    "ssl_model = DINO(batch_size_per_device=config.batch_size, num_classes=1000).to(device)\n",
    "ssl_model.load_state_dict(\n",
    "    torch.load(\"/content/epoch=99-step=1000900.ckpt\")[\"state_dict\"]\n",
    ")"
   ],
   "metadata": {
    "id": "VMs9mNDL4BUU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ—‚ï¸ Creating the Image Index"
   ],
   "metadata": {
    "id": "Hhr_-zGIntNL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transforms = T.Compose(\n",
    "    [T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])]\n",
    ")\n",
    "\n",
    "\n",
    "def augment(img: Image, transforms=transforms) -> torch.Tensor:\n",
    "    if img.mode == \"L\":\n",
    "        # Convert grayscale image to RGB by duplicating the single channel three times\n",
    "        img = ImageOps.colorize(img, black=\"black\", white=\"white\")\n",
    "\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def create_index(hf_dataset) -> faiss.IndexFlatL2:\n",
    "    index = faiss.IndexFlatL2(2048)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, element in enumerate(tqdm(hf_dataset)):\n",
    "            embeddings = ssl_model(augment(element[\"image\"], transforms).to(device))\n",
    "            embedding = embeddings[0].cpu().numpy()\n",
    "            index.add(np.array(embedding).reshape(1, -1))\n",
    "\n",
    "    faiss.write_index(index, f\"{config.method}.index\")\n",
    "    return index\n",
    "\n",
    "\n",
    "data_index = create_index(dataset[\"train\"])"
   ],
   "metadata": {
    "id": "WJR_XCzIL0Wt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if config.upload_to_wandb:\n",
    "    run = wandb.init(\n",
    "        project=\"SSL-Image-Retrieval\",\n",
    "        job_type=\"upload-embeddings\",\n",
    "        group=config.method,\n",
    "        config=config.to_dict(),\n",
    "        entity=config.wandb_entity,\n",
    "    )\n",
    "\n",
    "    embeddings_artifact = wandb.Artifact(\n",
    "        name=f\"{config.method}-embeddings\", type=\"faiss-embeddings\"\n",
    "    )\n",
    "    embeddings_artifact.add_file(local_path=f\"{config.method}.index\")\n",
    "    run.log_artifact(embeddings_artifact)\n",
    "\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "id": "EUjS4ftjPAH3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ”Ž Using the Image Index"
   ],
   "metadata": {
    "id": "D49Jk-QBPNYC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "api = wandb.Api()\n",
    "artifact = api.artifact(\"sauravmaheshkar/SSL-Image-Retrieval/dino-embeddings:latest\")\n",
    "artifact.download()\n",
    "\n",
    "data_index = read_index(\"/content/artifacts/dino-embeddings:v0/dino.index\")"
   ],
   "metadata": {
    "id": "84EO0J1pvlHI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def search_index(\n",
    "    index: faiss.IndexFlatL2, embedding: list, k: int = config.top_k\n",
    ") -> list:\n",
    "    _, I = index.search(np.array(embedding[0].reshape(1, -1)), k)\n",
    "\n",
    "    return I[0]"
   ],
   "metadata": {
    "id": "JOKCOUw2WMDG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from PIL import ImageOps\n",
    "\n",
    "search_idx = 11234  # @param {type: \"number\"}\n",
    "img = dataset[\"train\"][search_idx][\"image\"]\n",
    "\n",
    "# Define transforms\n",
    "transforms = T.Compose(\n",
    "    [T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])]\n",
    ")\n",
    "\n",
    "\n",
    "def augment(img: Image, transforms=transforms) -> torch.Tensor:\n",
    "    if img.mode == \"L\":\n",
    "        # Convert grayscale image to RGB by duplicating the single channel three times\n",
    "        img = ImageOps.colorize(img, black=\"black\", white=\"white\")\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Display images\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Plot the source image\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title(\"Query\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "# Get embeddings and retrieve similar images\n",
    "with torch.no_grad():\n",
    "    embedding = ssl_model(augment(img, transforms).to(device))\n",
    "    indices = search_index(data_index, np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "\n",
    "    for i, index in enumerate(indices[:3]):  # Get the first 3 retrieved images\n",
    "        retrieved_img = dataset[\"train\"][int(index)][\"image\"]\n",
    "        axs[i + 1].imshow(retrieved_img)\n",
    "        axs[i + 1].set_title(f\"Retrieved image {i + 1}\")\n",
    "        axs[i + 1].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "QKBwRra_I6tk"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
