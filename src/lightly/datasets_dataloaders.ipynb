{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgpBW4uIy55O"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "%%capture\n",
    "!pip install -qU albumentations datasets lightly\"[video]\"\n",
    "!git clone https://github.com/alexeygrigorev/clothing-dataset.git\n",
    "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-320.tgz\n",
    "!wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms import SimCLRTransform\n",
    "\n",
    "transform = SimCLRTransform()\n",
    "\n",
    "with tarfile.open(\"/content/imagewoof2-320.tgz\") as f:\n",
    "    f.extractall(\"/content/imagewoof2-320\")\n",
    "\n",
    "with tarfile.open(\"/content/ucf101_top5.tar.gz\") as f:\n",
    "    f.extractall(\"/content/ucf101_top5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ”¥ PyTorch Datasets"
   ],
   "metadata": {
    "id": "dgICGPIqzB-l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "\n",
    "base = torchvision.datasets.CIFAR10(root=\"data/torchvision/\", download=True)\n",
    "torch_dataset = LightlyDataset.from_torch_dataset(base, transform=transform)"
   ],
   "metadata": {
    "id": "U-01oNeHzECc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "## Torchvision Transforms\n",
    "torchvision_transforms = T.Compose(\n",
    "    [\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Albumentation Transforms\n",
    "albumentation_transforms = A.Compose(\n",
    "    [\n",
    "        A.CenterCrop(height=128, width=128),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Lightly Transforms\n",
    "lightly_transform = SimCLRTransform()\n",
    "\n",
    "\n",
    "torchvision_aug_dataset = LightlyDataset.from_torch_dataset(\n",
    "    base, transform=torchvision_transforms\n",
    ")\n",
    "albumentation_aug_dataset = LightlyDataset.from_torch_dataset(\n",
    "    base, transform=albumentation_transforms\n",
    ")\n",
    "lightly_aug_dataset = LightlyDataset.from_torch_dataset(\n",
    "    base, transform=lightly_transform\n",
    ")"
   ],
   "metadata": {
    "id": "Sb6IQ9bCgNz9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ¤— HuggingFace Datasets"
   ],
   "metadata": {
    "id": "OJV4MT-a2kzr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://huggingface.co/front/assets/homepage/hugs.svg)"
   ],
   "metadata": {
    "id": "RaP5lOD2vlMG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "base = load_dataset(\"uoft-cs/cifar10\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "def apply_transform(example_batch, transform=transform):\n",
    "    \"\"\"\n",
    "    Apply the given transform across a batch. To be used in a 'map' like manner\n",
    "\n",
    "    Args:\n",
    "      example_batch (Dict): a batch of data, should contain the key 'image'\n",
    "      tranform (Callable): image transformations to be performed\n",
    "\n",
    "    Returns:\n",
    "      updated batch with transformations applied to the image\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        \"image\" in example_batch.keys()\n",
    "    ), \"batch should be of type Dict[str, Any] with a key 'image'\"\n",
    "\n",
    "    example_batch[\"image\"] = [\n",
    "        transform(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "base.set_transform(apply_transform)\n",
    "\n",
    "hf_dataloader = torch.utils.data.DataLoader(base[\"train\"])"
   ],
   "metadata": {
    "id": "bKZX2BFb2690"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightlyDataset format"
   ],
   "metadata": {
    "id": "fhAMt2axvIQw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unlabelled ImageÂ Datasets"
   ],
   "metadata": {
    "id": "Z7vftlNQvJPm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "unlabelled_image_dataset = LightlyDataset(\n",
    "    input_dir=\"/content/clothing-dataset/images\", transform=transform\n",
    ")"
   ],
   "metadata": {
    "id": "n7Ha3IVxvLT1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Labeled Image Datasets"
   ],
   "metadata": {
    "id": "3MY4bj4nwX5p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "labelled_image_dataset = LightlyDataset(\n",
    "    input_dir=\"/content/imagewoof2-320/imagewoof2-320/train\", transform=transform\n",
    ")"
   ],
   "metadata": {
    "id": "01JUTPwLxEq9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Video Datasets"
   ],
   "metadata": {
    "id": "vG7IXfsIx1u7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "video_dataset = LightlyDataset(\n",
    "    input_dir=\"/content/ucf101_top5/train\", transform=transform\n",
    ")"
   ],
   "metadata": {
    "id": "FA-zbIiSx4Ic"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
