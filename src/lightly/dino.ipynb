{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **TL;DR**\n",
    "\n",
    "Pretrain a Vision Transformer using DINO ([`lightly`](https://github.com/lightly-ai/lightly) implementation) on an arbitrary dataset for Image Retrieval using [`faiss`](https://github.com/facebookresearch/faiss) as a vector database."
   ],
   "metadata": {
    "id": "4S5P65xy3i3x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📦 Packages and Basic Setup\n",
    "---"
   ],
   "metadata": {
    "id": "Sssfm1GmkgmT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WK-uaF3eJ9ay"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "%%capture\n",
    "!pip install faiss-gpu lightning lightly datasets supervision wandb ml-collections -q\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import warnings\n",
    "from typing import Any, Callable, Dict, TypeAlias\n",
    "\n",
    "import cv2\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import supervision as sv\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from datasets import load_dataset\n",
    "from faiss import read_index, write_index\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "\n",
    "BATCH_TYPE: TypeAlias = Dict[str, Any]\n",
    "\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "key = userdata.get(\"W&B\")\n",
    "os.environ[\"WANDB_API_KEY\"] = key\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import ml_collections\n",
    "\n",
    "# @title ⚙️ Configuration\n",
    "import wandb\n",
    "\n",
    "\n",
    "def get_config() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.model: str = \"dino_vits8\"  # @param {type: \"string\"}\n",
    "    config.dataset: str = \"ethz/food101\"  # @param {type: \"string\"}\n",
    "    config.hidden_dim: int = 256  # @param {type: \"number\"}\n",
    "    config.bottleneck_dim: int = 64  # @param {type: \"number\"}\n",
    "    config.output_dim: int = 2048  # @param {type: \"number\"}\n",
    "    config.warmup_teacher_temp_epochs: int = 5  # @param {type: \"number\"}\n",
    "    config.lr: float = 0.001  # @param {type: \"number\"}\n",
    "    config.epochs: int = 10  # @param {type: \"number\"}\n",
    "    config.batch_size: int = 16  # @param {type: \"number\"}\n",
    "    config.top_k: int = 3  # @param {type: \"number\"}\n",
    "    config.random_seed: int = 42  # @param {type: \"number\"}\n",
    "    config.upload_to_wandb = True  # @param {type:\"boolean\"}\n",
    "    config.upload_to_hf = True  # @param {type:\"boolean\"}\n",
    "    config.hf_entity: str = \"sauravmaheshkar\"  # @param {type: \"string\"}\n",
    "    config.wandb_entity: str = \"sauravmaheshkar\"  # @param {type: \"string\"}\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"SSL-Image-Retrieval\",\n",
    "    job_type=\"train\",\n",
    "    group=config.model,\n",
    "    config=config.to_dict(),\n",
    "    entity=config.wandb_entity,\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "pl.seed_everything(config.random_seed)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "WJieMscJmgtX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 💿 The Dataset\n",
    "---\n",
    "\n",
    "In this particular example we use the `ethz/food101` dataset from the huggingface hub. This dataset contains 75750 images from 101 classes.\n",
    "\n",
    "* This snippet highlights the ease of use of off-the-shelf `lightly` transforms ([`DINOTransform`](https://docs.lightly.ai/self-supervised-learning/lightly.transforms.html#lightly.transforms.dino_transform.DINOTransform)) when used with the huggingface ecosytem.\n",
    "* No need to refactor images into a single folder, can simply iterate over all images in the dataset object."
   ],
   "metadata": {
    "id": "zIc9K3KJmmSV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "dataset = load_dataset(config.dataset, trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": true,
    "id": "Ki-pwvRSLAll"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform = DINOTransform()\n",
    "\n",
    "\n",
    "def apply_transform(\n",
    "    example_batch: BATCH_TYPE, transform: Callable = transform\n",
    ") -> BATCH_TYPE:\n",
    "    \"\"\"\n",
    "    Apply the given transform across a batch. To be used in a 'map' like manner\n",
    "\n",
    "    Args:\n",
    "      example_batch (Dict): a batch of data, should contain the key 'image'\n",
    "      tranform (Callable): image transformations to be performed\n",
    "\n",
    "    Returns:\n",
    "      updated batch with transformations applied to the image\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        \"image\" in example_batch.keys()\n",
    "    ), \"batch should be of type Dict[str, Any] with a key 'image'\"\n",
    "\n",
    "    example_batch[\"image\"] = [\n",
    "        transform(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "dataset.set_transform(apply_transform)"
   ],
   "metadata": {
    "id": "3jhmHZNxyxTM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    ")"
   ],
   "metadata": {
    "id": "3cQUTC6EzDvW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✍️ Model Architecture & Training\n",
    "---\n",
    "\n",
    "![](https://github.com/SauravMaheshkar/SauravMaheshkar/blob/main/assets/DINO/DINO.png?raw=true)"
   ],
   "metadata": {
    "id": "kk1S5GNowcmw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "\n",
    "\n",
    "class DINO(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = config.model,\n",
    "        hidden_dim: int = config.hidden_dim,\n",
    "        bottleneck_dim: int = config.bottleneck_dim,\n",
    "        output_dim: int = config.output_dim,\n",
    "        warmup_teacher_temp_epochs: int = config.warmup_teacher_temp_epochs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        backbone = torch.hub.load(\"facebookresearch/dino:main\", model, pretrained=False)\n",
    "        input_dim = backbone.embed_dim\n",
    "\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            bottleneck_dim=bottleneck_dim,\n",
    "            output_dim=output_dim,\n",
    "            freeze_last_layer=1,\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            bottleneck_dim=bottleneck_dim,\n",
    "            output_dim=output_dim,\n",
    "        )\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "\n",
    "ssl_model = DINO(\n",
    "    model=config.model,\n",
    "    hidden_dim=config.hidden_dim,\n",
    "    bottleneck_dim=config.bottleneck_dim,\n",
    "    output_dim=config.output_dim,\n",
    "    warmup_teacher_temp_epochs=config.warmup_teacher_temp_epochs,\n",
    ").to(device)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "VMs9mNDL4BUU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = DINOLoss(\n",
    "    output_dim=config.output_dim,\n",
    "    warmup_teacher_temp_epochs=config.warmup_teacher_temp_epochs,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=config.lr)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    total_loss = 0\n",
    "    momentum_val = cosine_schedule(epoch, config.epochs, 0.996, 1)\n",
    "    for batch in dataloader:\n",
    "        views = batch[\"image\"]\n",
    "        update_momentum(\n",
    "            ssl_model.student_backbone, ssl_model.teacher_backbone, m=momentum_val\n",
    "        )\n",
    "        update_momentum(ssl_model.student_head, ssl_model.teacher_head, m=momentum_val)\n",
    "        views = [view.to(device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [ssl_model.forward_teacher(view) for view in global_views]\n",
    "        student_out = [ssl_model.forward(view) for view in views]\n",
    "        loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "        ssl_model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    torch.save(\n",
    "        ssl_model.state_dict(),\n",
    "        f\"artifacts/dino-food101-{config.model}-{config.hidden_dim}\",\n",
    "    )\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    wandb.log({\"avg_loss\": avg_loss})\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "id": "vjHQMs9_ovDK"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
