{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAFi-4kdCmxt"
   },
   "source": [
    "## 📦 Packages and Basic Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VO1o7LbeBqA-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "torch_version = torch.__version__.split(\"+\")\n",
    "os.environ[\"TORCH\"] = torch_version[0]\n",
    "os.environ[\"CUDA\"] = torch_version[1]\n",
    "\n",
    "!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-geometric\n",
    "!pip install -q --upgrade wandb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "import wandb\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GMMConv\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSVa4JLLmlGR"
   },
   "outputs": [],
   "source": [
    "# @title ⚙ Configuration\n",
    "\n",
    "# Paste your api key here\n",
    "os.environ[\"WANDB_API_KEY\"] = \"...\"\n",
    "\n",
    "wandb.init(project=\"MoNet\", entity=\"graph-neural-networks\")\n",
    "\n",
    "config = wandb.config\n",
    "config.lr = 0.01  # @param {type: \"number\"}\n",
    "config.latent_dim = 16  # @param {type: \"number\"}\n",
    "config.num_epochs = 50  # @param {type: \"number\"}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eLKO479DjIw"
   },
   "source": [
    "## 💿 The Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKKG6GFLDkdU"
   },
   "outputs": [],
   "source": [
    "dataset = Planetoid(\"data/\", \"Cora\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SAwbXWs8Qr5"
   },
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    row, col = data.edge_index\n",
    "    deg = degree(col, data.num_nodes)\n",
    "    data.edge_attr = torch.stack(\n",
    "        [1 / torch.sqrt(deg[row]), 1 / torch.sqrt(deg[col])], dim=-1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZo_9RtwDDC9"
   },
   "source": [
    "## ✍️ Model Architecture & Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "026XzX-FDDvM"
   },
   "outputs": [],
   "source": [
    "class MoNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GMMConv(in_channels, hidden_channels, dim=2, kernel_size=16)\n",
    "        self.conv2 = GMMConv(hidden_channels, out_channels, dim=2, kernel_size=16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = MoNet(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=config.latent_dim,\n",
    "    out_channels=dataset.num_classes,\n",
    ")\n",
    "\n",
    "model, data = model.to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAHczXZFOqbz"
   },
   "source": [
    "## Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSCTkgOfJq0-"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ptce7SKM5dho"
   },
   "outputs": [],
   "source": [
    "best_val_acc = final_test_acc = 0\n",
    "\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    wandb.log({\"Training Loss\": loss, \"Training Accuracy\": train_acc})\n",
    "\n",
    "wandb.run.summary[\"Training Loss\"] = loss\n",
    "wandb.run.summary[\"Training Accuracy\"] = train_acc\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNdEk2wPsoj6Kh355iyDvpA",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
